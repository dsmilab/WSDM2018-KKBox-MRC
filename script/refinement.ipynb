{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#data.py\" data-toc-modified-id=\"data.py-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>data.py</a></span><ul class=\"toc-item\"><li><span><a href=\"#library\" data-toc-modified-id=\"library-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>library</a></span></li><li><span><a href=\"#ColumnSelector\" data-toc-modified-id=\"ColumnSelector-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>ColumnSelector</a></span></li><li><span><a href=\"#KKboxRSDataset\" data-toc-modified-id=\"KKboxRSDataset-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>KKboxRSDataset</a></span></li><li><span><a href=\"#FeatureProcessor\" data-toc-modified-id=\"FeatureProcessor-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>FeatureProcessor</a></span></li><li><span><a href=\"#ImplicitProcessor\" data-toc-modified-id=\"ImplicitProcessor-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>ImplicitProcessor</a></span></li></ul></li><li><span><a href=\"#cf_lgbm.py\" data-toc-modified-id=\"cf_lgbm.py-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>cf_lgbm.py</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:41:37.983955Z",
     "start_time": "2017-12-07T10:41:37.969078Z"
    }
   },
   "source": [
    "# data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:45:33.229117Z",
     "start_time": "2017-12-07T10:45:32.482720Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.sparse import coo_matrix, linalg\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColumnSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:45:33.237796Z",
     "start_time": "2017-12-07T10:45:33.230877Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns].to_dict(orient='record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KKboxRSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:45:33.281970Z",
     "start_time": "2017-12-07T10:45:33.239452Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KKboxRSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train=True, processor=None):\n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = processor.load(train=self.train)\n",
    "        else:\n",
    "            self.test_data, self.test_labels = processor.load(train=self.train)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (vectors, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            vectors, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            vectors, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        return vectors, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:46:46.233320Z",
     "start_time": "2017-12-07T10:46:40.372262Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureProcessor(object):\n",
    "    songs_file = 'songs.csv'\n",
    "    extra_file = 'song_extra_info.csv'\n",
    "    members_file = 'members.csv'\n",
    "    train_file = 'train.csv'\n",
    "    test_file = 'test.csv'\n",
    "\n",
    "    def __init__(self, root='./data'):\n",
    "\n",
    "        assert os.path.exists(root), '%s not exists!' % root\n",
    "        self.root = os.path.expanduser(root)\n",
    "        train, test = self._load_raw()\n",
    "        self._process_member()\n",
    "        self._process_extra()\n",
    "        self._process_songs()\n",
    "\n",
    "        train = self._preprocess(train)\n",
    "        test = self._preprocess(test)\n",
    "\n",
    "        self._mean_song_length = np.mean(train['song_length'])\n",
    "\n",
    "        # number of times a song has been played before\n",
    "        self._dict_count_song_played_train = {k: v for k, v in train['song_id'].value_counts().items()}\n",
    "        self._dict_count_song_played_test = {k: v for k, v in test['song_id'].value_counts().items()}\n",
    "\n",
    "        # number of times the artist has been played\n",
    "        self._dict_count_artist_played_train = {k: v for k, v in train['artist_name'].value_counts().items()}\n",
    "        self._dict_count_artist_played_test = {k: v for k, v in test['artist_name'].value_counts().items()}\n",
    "\n",
    "        train['count_artist_played'] = train['artist_name'].apply(self._count_artist_played).astype(np.int64)\n",
    "        test['count_artist_played'] = test['artist_name'].apply(self._count_artist_played).astype(np.int64)\n",
    "\n",
    "        train['count_song_played'] = train['song_id'].apply(self._count_song_played).astype(np.int64)\n",
    "        test['count_song_played'] = test['song_id'].apply(self._count_song_played).astype(np.int64)\n",
    "\n",
    "        train = self._add_new_feature(train, True)\n",
    "        test = self._add_new_feature(test, False)\n",
    "\n",
    "        track_count_df = train[['song_id', 'artist_name']].drop_duplicates('song_id')\n",
    "        track_count_df = track_count_df.groupby('artist_name').agg('count').reset_index()\n",
    "        track_count_df.columns = ['artist_name', 'track_count']\n",
    "        track_count_df = track_count_df.sort_values('track_count', ascending=False)\n",
    "\n",
    "        train = self._add_comb_feature(train, test, track_count_df, is_train=True)\n",
    "        test = self._add_comb_feature(train, test, track_count_df, is_train=False)\n",
    "\n",
    "        # total_genre_ids = pd.concat([train.genre_ids, test.genre_ids])\n",
    "        # self.genres = np.unique('|'.join(total_genre_ids).split('|'))\n",
    "\n",
    "        # train = self._add_genre_feature(train)\n",
    "        # test = self._add_genre_feature(test)\n",
    "\n",
    "        train.fillna('na_for_later', inplace=True)\n",
    "        test.fillna('na_for_later', inplace=True)\n",
    "\n",
    "        for col in train.columns:\n",
    "            if train[col].dtype == object:\n",
    "                train[col] = train[col].astype('category')\n",
    "                test[col] = test[col].astype('category')\n",
    "\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "        self._compute_msno_song_similarity(train, test)\n",
    "\n",
    "    def load(self):\n",
    "        return self.train, self.test, self.unknown_msno_map, self.unknown_song_map\n",
    "\n",
    "    def _load_raw(self):\n",
    "        start = time.time()\n",
    "\n",
    "        self.songs = pd.read_csv(os.path.join(self.root, self.songs_file))\n",
    "        self.extra = pd.read_csv(os.path.join(self.root, self.extra_file))\n",
    "        self.members = pd.read_csv(os.path.join(self.root, self.members_file),\n",
    "                                   parse_dates=['registration_init_time','expiration_date'])\n",
    "\n",
    "        train_raw = pd.read_csv(\n",
    "                os.path.join(self.root, self.train_file))\n",
    "\n",
    "        test_raw = pd.read_csv(\n",
    "                os.path.join(self.root, self.test_file))\n",
    "\n",
    "        logging.debug(\"load raw data in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        return train_raw, test_raw\n",
    "\n",
    "    def _preprocess(self, df):\n",
    "\n",
    "        start = time.time()\n",
    "        df = df.merge(self.songs, on='song_id', how='left')\n",
    "        df = df.merge(self.members, on='msno', how='left')\n",
    "        df = df.merge(self.extra, on='song_id', how='left')\n",
    "\n",
    "        # howeverforever\n",
    "        df['source_system_tab'].fillna('others', inplace=True)\n",
    "        df['source_screen_name'].fillna('others', inplace=True)\n",
    "        df['source_type'].fillna('nan', inplace=True)\n",
    "\n",
    "        df.song_length.fillna(200000, inplace=True)\n",
    "        logging.debug(\"preprocess in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_genre_feature(self, df):\n",
    "\n",
    "        start = time.time()\n",
    "        genre_ids = df.genre_ids.apply(lambda x: x.split('|'))\n",
    "        for g in self.genres:\n",
    "            df['g_' + g] = genre_ids.apply(lambda x: self._find_genre(x, g))\n",
    "\n",
    "        logging.debug(\"add genre features in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_comb_feature(self, train, test, track_count_df, is_train=True):\n",
    "\n",
    "        start = time.time()\n",
    "        if is_train:\n",
    "            self.count_df = train['song_id'].value_counts().reset_index()\n",
    "            artist_count_df = train[['artist_name', 'target']].groupby('artist_name').agg(\n",
    "                ['mean', 'count']).reset_index()\n",
    "            df = train\n",
    "        else:\n",
    "            comb_df = train.append(test)\n",
    "            self.count_df = comb_df['song_id'].value_counts().reset_index()\n",
    "            artist_count_df = comb_df[['artist_name', 'target']].groupby('artist_name').agg(\n",
    "                ['mean', 'count']).reset_index()\n",
    "\n",
    "            df = test\n",
    "\n",
    "        self.count_df.columns = ['song_id', 'play_count']\n",
    "\n",
    "        df = df.merge(self.count_df, on='song_id', how='left')\n",
    "        df['play_count'].fillna(0, inplace=True)\n",
    "\n",
    "        artist_count_df.columns = ['artist_name', 'replay_pb', 'play_count']\n",
    "\n",
    "        artist_count_df = artist_count_df.merge(\n",
    "            track_count_df, on='artist_name', how='left')\n",
    "        artist_count_df['track_count'].fillna(0, inplace=True)\n",
    "\n",
    "        df = df.merge(\n",
    "            artist_count_df[['artist_name', 'track_count']],\n",
    "            on='artist_name',\n",
    "            how='left')\n",
    "\n",
    "        logging.debug(\"add comb features in %0.2fs\" % (time.time() - start))\n",
    "        return df\n",
    "\n",
    "    def _add_new_feature(self, df, is_train=True):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # howeverforever\n",
    "        df['source_merged'] = df['source_system_tab'].map(str) + ' | ' + df['source_screen_name'].map(str) + ' | ' + df['source_type'].map(str)\n",
    "\n",
    "        if is_train:\n",
    "            self.count_df = df[['source_merged', 'target']].groupby('source_merged').agg(['mean', 'count'])\n",
    "            self.count_df.reset_index(inplace=True)\n",
    "            self.count_df.columns = ['source_merged', 'source_replay_pb', 'source_replay_count']\n",
    "\n",
    "        df = df.merge(self.count_df, on='source_merged', how='left')\n",
    "\n",
    "        df['1h_source'] = df['source_replay_pb'].apply(self._one_hot_encode_source)\n",
    "        df.drop(['source_merged', 'source_replay_pb', 'source_replay_count'], axis=1, inplace=True)\n",
    "\n",
    "        df['1h_system_tab'] = df['source_system_tab'].apply(self._one_hot_encode_system_tab)\n",
    "        df['1h_screen_name'] = df['source_screen_name'].apply(self._one_hot_encode_screen_name)\n",
    "        df['1h_source_type'] = df['source_type'].apply(self._one_hot_encode_source_type)\n",
    "\n",
    "        df['smaller_song'] = df['song_length'].apply(self._smaller_song).astype(np.int8)\n",
    "\n",
    "        df['is_2017'] = df['song_year'].apply(self._is_2017).astype(np.int8)\n",
    "\n",
    "        logging.debug(\"add new features in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _process_songs(self):\n",
    "        self.songs['artist_name'].fillna('no_artist',inplace=True)\n",
    "        self.songs['is_featured'] = self.songs['artist_name'].apply(self._is_featured).astype(np.int8)\n",
    "\n",
    "        self.songs['artist_count'] = self.songs['artist_name'].apply(self._artist_count).astype(np.int8)\n",
    "        self.songs['artist_composer'] = (self.songs['artist_name'] == self.songs['composer']).astype(np.int8)\n",
    "\n",
    "        # if artist, lyricist and composer are all three same\n",
    "        self.songs['artist_composer_lyricist'] = ((self.songs['artist_name'] == self.songs['composer']) &\n",
    "                                                  (self.songs['artist_name'] == self.songs['lyricist']) &\n",
    "                                                  (self.songs['composer'] == self.songs['lyricist'])).astype(np.int8)\n",
    "\n",
    "        self.songs['song_lang_boolean'] = self.songs['language'].apply(self._song_lang_boolean).astype(np.int8)\n",
    "\n",
    "        # howeverforever\n",
    "        self.songs['genre_count'] = self.songs['genre_ids'].apply(self._parse_splitted_category_to_number)\n",
    "        self.songs['composer_count'] = self.songs['composer'].apply(self._parse_splitted_category_to_number)\n",
    "        self.songs['lyricist_count'] = self.songs['lyricist'].apply(self._parse_splitted_category_to_number)\n",
    "\n",
    "        self.songs['1h_lang'] = self.songs['language'].apply(self._one_hot_encode_lang)\n",
    "\n",
    "        self.songs['1h_song_length'] = self.songs['song_length'].apply(lambda x: 1 if x <= 239738 else 0)\n",
    "\n",
    "        self.songs['language'].fillna('nan', inplace=True)\n",
    "        self.songs['composer'].fillna('nan', inplace=True)\n",
    "        self.songs['lyricist'].fillna('nan', inplace=True)\n",
    "        self.songs['genre_ids'].fillna('nan', inplace=True)\n",
    "        # self.songs.drop(['language'], axis=1, inplace=True)\n",
    "        assert(~self.songs.isnull().any().any())\n",
    "\n",
    "    def _process_member(self):\n",
    "\n",
    "        self.members['membership_days'] = self.members['expiration_date'].subtract(self.members['registration_init_time']).dt.days.astype(int)\n",
    "\n",
    "        self.members['registration_year'] = self.members['registration_init_time'].dt.year\n",
    "        self.members['registration_month'] = self.members['registration_init_time'].dt.month\n",
    "        self.members['registration_date'] = self.members['registration_init_time'].dt.day\n",
    "\n",
    "        self.members['expiration_year'] = self.members['expiration_date'].dt.year\n",
    "        self.members['expiration_month'] = self.members['expiration_date'].dt.month\n",
    "        self.members['expiration_date'] = self.members['expiration_date'].dt.day\n",
    "        self.members = self.members.drop(['registration_init_time'], axis=1)\n",
    "\n",
    "        # howeverforever\n",
    "        self.members['bd'] = self.members['bd'].apply(self._transform_bd_outliers)\n",
    "        self.members['gender'].fillna('nan', inplace=True)\n",
    "        self.members['1h_via'] = self.members['registered_via'].apply(self._one_hot_encode_via)\n",
    "        assert(~self.members.isnull().any().any())\n",
    "\n",
    "    def _process_extra(self):\n",
    "        self.extra['song_year'] = self.extra['isrc'].apply(self._transform_isrc_to_year)\n",
    "        self.extra.drop(['name', 'isrc'], axis=1, inplace=True)\n",
    "\n",
    "        # howeverforever\n",
    "        # self.extra['song_country'] = self.extra['isrc'].apply(self._transform_isrc_to_country)\n",
    "        # self.extra['song_registration'] = self.extra['isrc'].apply(self._transform_isrc_to_reg)\n",
    "        # self.extra['song_designation'] = self.extra['isrc'].apply(self._transfrom_isrc_to_desig)\n",
    "\n",
    "        self.extra['1h_song_year'] = self.extra['song_year'].apply(self._one_hot_encode_year)\n",
    "        # self.extra['1h_song_country'] = self.extra['song_country'].apply(self._one_hot_encode_country)\n",
    "\n",
    "        self.extra['song_year'].fillna(2017, inplace=True)\n",
    "        # self.extra['song_registration'].fillna('***', inplace=True)\n",
    "\n",
    "        assert(~self.extra.isnull().any().any())\n",
    "\n",
    "    def _compute_msno_song_similarity(self, train, test):\n",
    "\n",
    "        start = time.time()\n",
    "        member_feature = ['city',\n",
    "                          'bd',\n",
    "                          'gender',\n",
    "                          'registered_via',\n",
    "                          'expiration_date',\n",
    "                          'membership_days',\n",
    "                          'registration_year',\n",
    "                          'registration_month',\n",
    "                          'registration_date',\n",
    "                          'expiration_year',\n",
    "                          'expiration_month']\n",
    "\n",
    "        song_feature = ['genre_ids',\n",
    "                        'artist_name',\n",
    "                        'language',\n",
    "                        'composer',\n",
    "                        'lyricist',\n",
    "                        'song_year']\n",
    "\n",
    "        member_pipeline = Pipeline([\n",
    "                ('extract', ColumnSelector(member_feature)),\n",
    "                ('dicVect', DictVectorizer())])\n",
    "        song_pipeline = Pipeline([\n",
    "                ('extract', ColumnSelector(song_feature)),\n",
    "                ('dicVect', DictVectorizer())])\n",
    "\n",
    "        songs = self.songs.merge(self.extra, on = 'song_id', how = 'left').fillna('test')\n",
    "        members = self.members.fillna('test')\n",
    "        self.msno_x = {v: i for i, v in enumerate(members.msno)}\n",
    "        self.song_x = {v: i for i, v in enumerate(songs.song_id)}\n",
    "\n",
    "        self.msno_m = member_pipeline.fit_transform(members)\n",
    "        logging.debug(\"transform members in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        self.song_m = song_pipeline.fit_transform(songs)\n",
    "        logging.debug(\"transform songs in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        known_msno = set(train.msno.unique())\n",
    "        known_song = set(train.song_id.unique())\n",
    "\n",
    "        unknown_msno = list(set(test.msno.unique()) - known_msno)\n",
    "        total_msno = float(len(unknown_msno))\n",
    "        unknown_song = list(set(test.song_id.unique()) - known_song)\n",
    "        total_song = float(len(unknown_song))\n",
    "\n",
    "        self.unknown_msno_map, self.unknown_song_map = {}, {}\n",
    "\n",
    "        start = time.time()\n",
    "        known_msno_list = members.msno.apply(lambda x: x in known_msno)\n",
    "        known_song_list = songs.song_id.apply(lambda x: x in known_song)\n",
    "        logging.debug(\"establish known list in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "        # start = time.time()\n",
    "\n",
    "        # pool = mp.Pool(processes=6)\n",
    "\n",
    "        # Parallel(n_jobs=6)(delayed(self._get_unknown_map)(i, members.msno, known_msno_list, True) for i in unknown_msno)\n",
    "        # logging.debug(\"process msno in %0.2fs\" % (time.time() - start))\n",
    "        n = 0\n",
    "        for i in unknown_msno:\n",
    "            if i in self.msno_x:\n",
    "                df = self._get_rank(self.msno_m, self.msno_x[i], members.msno, known_msno_list)\n",
    "                self.unknown_msno_map[i] = df.iloc[0]['id']\n",
    "            # else:\n",
    "            #     self.unknown_msno_map[i] = 'new'\n",
    "            n += 1\n",
    "            if (n + 1) % 100 == 0: print('msno: %f %%' % ((n/total_msno) * 100))\n",
    "\n",
    "        # for i in unknown_song:\n",
    "            # self.unknown_song_map[i] = 'new'\n",
    "        # start = time.time()\n",
    "        # r = Parallel(n_jobs=-1, verbose=100)(delayed(self._get_unknown_map)(i) for i in unknown_song)\n",
    "        # logging.debug(\"difficult part in %0.2fs\" % (time.time() - start))\n",
    "        # for k, v in zip(unknown_song, r):\n",
    "        #     self.unknown_song_map[k] = v\n",
    "        # n = 0\n",
    "        # for i in unknown_song:\n",
    "        #     if i in self.song_x:\n",
    "        #         df = self._get_rank(self.song_m, self.song_x[i], songs.song_id, known_song_list)\n",
    "        #         self.unknown_song_map[i] = df.iloc[0]['id']\n",
    "        # #     else:\n",
    "        # #         unknown_song_map[i] = 'new'\n",
    "        #     n += 1\n",
    "        #     if (n + 1) % 100 == 0: print('song: %f %%' % ((n/total_song) * 100))\n",
    "\n",
    "        logging.debug(\"transform all unknown data in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "    # def _get_unknown_map(self, i, map_list, known_list, msno=True):\n",
    "    # def _get_unknown_map(self, i):\n",
    "        # if msno:\n",
    "        #     if i in self.msno_x:\n",
    "        #         df = self._get_rank(self.msno_m, self.msno_x[i], map_list, known_list)\n",
    "        #         self.unknown_msno_map[i] = df.iloc[0]['id']\n",
    "        #     # else:\n",
    "        #     #     self.unknown_msno_map[i] = 'new'\n",
    "        # else:\n",
    "        # if i in self.song_x:\n",
    "        #     # df = self._get_rank(self.song_m, self.song_x[i], map_list, known_list)\n",
    "        #     df = self._get_rank(self.song_m, self.song_x[i], self.test_a, self.test_b)\n",
    "        #     return df.iloc[0]['id']\n",
    "            # else:\n",
    "            #     self.unknown_song_map[i] = 'new'\n",
    "\n",
    "    def _get_rank(self, model, w, id_list, known_list):\n",
    "        result = cosine_similarity(model, model[w].toarray().reshape(1, -1)).reshape(1, -1)[0]\n",
    "        r = pd.DataFrame({'id': id_list, 'similarity': result, 'known': known_list})\n",
    "        return r[r.known].sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def _transform_two_dates_to_days(self, row):\n",
    "        start = parse_str_to_date(row['registration_init_time'])\n",
    "        end = parse_str_to_date(row['expiration_date'])\n",
    "        delta = end - start\n",
    "        return delta.days\n",
    "\n",
    "    def _transform_outliers(self, x, mean, std):\n",
    "        return x if np.abs(x - mean) <= 3 * std else -1\n",
    "\n",
    "    def _transform_init_time_to_ym(self, time):\n",
    "        time_str = str(time)\n",
    "        year = int(time_str[:4])\n",
    "        month = int(time_str[4:6])\n",
    "        return int(\"%04d%02d\" % (year, month))\n",
    "\n",
    "    def _transform_bd_outliers(self, bd):\n",
    "        # figure is from \"exploration\"\n",
    "        if bd >= 120 or bd <= 7:\n",
    "            return 'nan'\n",
    "        mean = 28.99737187910644\n",
    "        std = 9.538470787507382\n",
    "        return bd if abs(bd - mean) <= 3 * std else 'nan'\n",
    "\n",
    "    def _parse_splitted_category_to_number(self, x):\n",
    "        if x is np.nan:\n",
    "            return 0\n",
    "        x = str(x)\n",
    "        x.replace('/', '|')\n",
    "        x.replace(';', '|')\n",
    "        x.replace('\\\\', '|')\n",
    "        x.replace(' and ', '|')\n",
    "        x.replace('&', '|')\n",
    "        x.replace('+', '|')\n",
    "        return x.count('|') + 1\n",
    "\n",
    "    def _one_hot_encode_year(self, x):\n",
    "        return 1 if 2013 <= float(x) <= 2017 else 0\n",
    "\n",
    "    def _one_hot_encode_country(self, x):\n",
    "        return 1 if x == 'TW' or x == 'CN' or x == 'HK' else 0\n",
    "\n",
    "    def _one_hot_encode_via(self, x):\n",
    "        return 0 if x == 4 else 1\n",
    "\n",
    "    def _one_hot_encode_screen_name(self, x):\n",
    "        return 1 if x == 'Local playlist more' or x == 'My library' else 0\n",
    "\n",
    "    def _one_hot_encode_system_tab(self, x):\n",
    "        return 1 if x == 'my library' else 0\n",
    "\n",
    "    def _one_hot_encode_source_type(self, x):\n",
    "        return 1 if x == 'local-library' or x == 'local-playlist' else 0\n",
    "\n",
    "    def _one_hot_encode_source(self, x):\n",
    "        return 1 if x >= 0.6 else 0\n",
    "\n",
    "    def _one_hot_encode_lang(self, x):\n",
    "        return 1 if x in [-1, 17, 45] else 0\n",
    "\n",
    "    def _transform_isrc_to_year(self, isrc):\n",
    "        if type(isrc) != str:\n",
    "            return np.nan\n",
    "        # this year 2017\n",
    "        suffix = int(isrc[5:7])\n",
    "        return 1900 + suffix if suffix > 17 else 2000 + suffix\n",
    "\n",
    "    def _genre_id_count(self, x):\n",
    "        if x == 'no_genre_id':\n",
    "            return 0\n",
    "        else:\n",
    "            return x.count('|') + 1\n",
    "\n",
    "    def _lyricist_count(self, x):\n",
    "        if x == 'no_lyricist':\n",
    "            return 0\n",
    "        else:\n",
    "            return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n",
    "        return sum(map(x.count, ['|', '/', '\\\\', ';']))\n",
    "\n",
    "    def _composer_count(self, x):\n",
    "        if x == 'no_composer':\n",
    "            return 0\n",
    "        else:\n",
    "            return sum(map(x.count, ['|', '/', '\\\\', ';'])) + 1\n",
    "\n",
    "    def _is_featured(self, x):\n",
    "        if 'feat' in str(x) :\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def _artist_count(self, x):\n",
    "        if x == 'no_artist':\n",
    "            return 0\n",
    "        else:\n",
    "            return x.count('and') + x.count(',') + x.count('feat') + x.count('&')\n",
    "\n",
    "    def _song_lang_boolean(self, x):\n",
    "        # is song language 17 or 45.\n",
    "        if '17.0' in str(x) or '45.0' in str(x):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def _is_2017(self, x):\n",
    "        if x == 2017.0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def _smaller_song(self, x):\n",
    "        if x < self._mean_song_length:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def _count_song_played(self, x):\n",
    "        try:\n",
    "            return self._dict_count_song_played_train[x]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return self._dict_count_song_played_test[x]\n",
    "            except KeyError:\n",
    "                return 0\n",
    "\n",
    "    def _count_artist_played(self, x):\n",
    "        try:\n",
    "            return self._dict_count_artist_played_train[x]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return self._dict_count_artist_played_test[x]\n",
    "            except KeyError:\n",
    "                return 0\n",
    "\n",
    "    def _find_genre(self, g_list, g):\n",
    "        return True if g in g_list else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImplicitProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:45:40.047744Z",
     "start_time": "2017-12-07T10:45:39.176066Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ImplicitProcessor(object):\n",
    "\n",
    "    def __init__(self, feature_size=100, calculate_training_loss=False, save_dir='./model',\n",
    "                 iterations=15, n_clusters=30, random_state=50, cluster=True):\n",
    "\n",
    "        assert os.path.exists(save_dir), '%s not exists!' % save_dir\n",
    "        self.save_dir = os.path.expanduser(save_dir)\n",
    "\n",
    "        assert feature_size % 2 == 0, 'feature_size need to be an even number!'\n",
    "        self.factors = int(feature_size / 2)\n",
    "\n",
    "        self.calculate_training_loss = calculate_training_loss\n",
    "        self.iterations = iterations\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.cluster = cluster\n",
    "\n",
    "    def fit(self, train_df, test_df, unknown_msno_map, unknown_song_map):\n",
    "\n",
    "        self.train_raw = train_df[['msno', 'song_id', 'target']]\n",
    "        self.test_raw = test_df[['msno', 'song_id']]\n",
    "\n",
    "        self.unknown_msno_map = unknown_msno_map\n",
    "        self.unknown_song_map = unknown_song_map\n",
    "\n",
    "        self._process_train()\n",
    "        self._process_test()\n",
    "\n",
    "        self._fit_model(self.calculate_training_loss, self.iterations)\n",
    "\n",
    "        train_factors = self._get_factors(is_train=True)\n",
    "        test_factors = self._get_factors(is_train=False)\n",
    "\n",
    "        if self.cluster:\n",
    "\n",
    "            self._get_clusting_feature(self.n_clusters, self.random_state)\n",
    "            train_group = self._get_group(is_train=True)\n",
    "            test_group = self._get_group(is_train=False)\n",
    "\n",
    "            train_add_feature = pd.concat([train_group, train_factors], axis=1, ignore_index=True)\n",
    "            test_add_feature = pd.concat([test_group, test_factors], axis=1, ignore_index=True)\n",
    "        else:\n",
    "            train_add_feature = train_factors\n",
    "            test_add_feature = test_factors\n",
    "\n",
    "        y_train = train_df['target'].values\n",
    "        train_df = train_df.drop(['target'], axis=1)\n",
    "        # train_df = train_df.drop(['msno', 'song_id', 'target'], axis=1)\n",
    "\n",
    "        ids = test_df['id'].values\n",
    "        test_df = test_df.drop(['id'], axis=1)\n",
    "        # test_df = test_df.drop(['msno', 'song_id', 'id'], axis=1)\n",
    "\n",
    "        X_train = pd.concat([train_df, train_add_feature], axis=1, ignore_index=True)\n",
    "        X_test = pd.concat([test_df, test_add_feature], axis=1, ignore_index=True)\n",
    "\n",
    "        return X_train, y_train, X_test, ids\n",
    "\n",
    "    def _get_clusting_feature(self, n_clusters, random_state):\n",
    "\n",
    "        start = time.time()\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "\n",
    "        self.item_group = kmeans.fit_predict(self.item_factors)\n",
    "        self.user_group = kmeans.fit_predict(self.user_factors)\n",
    "        logging.debug(\"add clustering feature in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "    def _process_train(self):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        self.msno_list = list(self.train_raw.msno.unique())\n",
    "        self.song_list = list(self.train_raw.song_id.unique())\n",
    "\n",
    "        self.msno_ix = {v: i for i, v in enumerate(self.msno_list)}\n",
    "        self.song_ix = {v: i for i, v in enumerate(self.song_list)}\n",
    "\n",
    "        self.train_raw['msno_ix'] = self.train_raw.msno.apply(\n",
    "            lambda x: self.msno_ix[x]).astype(\"category\")\n",
    "\n",
    "        self.train_raw['song_ix'] = self.train_raw.song_id.apply(\n",
    "            lambda x: self.song_ix[x]).astype(\"category\")\n",
    "\n",
    "        # self.train_raw = self.train_raw[self.train_raw.target == 1]\n",
    "        self.targets = coo_matrix((self.train_raw['target'].astype(float),\n",
    "                                  (self.train_raw['song_ix'].cat.codes,\n",
    "                                   self.train_raw['msno_ix'].cat.codes)))\n",
    "\n",
    "        pickle.dump(self.msno_list, open(os.path.join(self.save_dir, 'msno_list.pkl'), 'wb'))\n",
    "        pickle.dump(self.song_list, open(os.path.join(self.save_dir, 'song_list.pkl'), 'wb'))\n",
    "\n",
    "        logging.debug(\"preprocess train data in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "    def _get_ix(self, x, msno=True):\n",
    "        if msno:\n",
    "            if x in self.msno_ix:\n",
    "                return self.msno_ix[x]\n",
    "            elif x in self.unknown_msno_map:\n",
    "                return self.msno_ix[self.unknown_msno_map[x]]\n",
    "            else:\n",
    "                return 'new'\n",
    "        else:\n",
    "            if x in self.song_ix:\n",
    "                return self.song_ix[x]\n",
    "            elif x in self.unknown_song_map:\n",
    "                return self.song_ix[self.unknown_song_map[x]]\n",
    "            else:\n",
    "                return 'new'\n",
    "\n",
    "    def _process_test(self):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        self.test_raw['msno_ix'] = self.test_raw.msno.apply(lambda x: self._get_ix(x, True))\n",
    "            # lambda x: self.msno_ix[x] if x in self.msno_ix.keys() else self.msno_ix[self.unknown_msno_map[x]]).astype(\"category\")\n",
    "\n",
    "        self.test_raw['song_ix'] = self.test_raw.song_id.apply(lambda x: self._get_ix(x, False))\n",
    "            # lambda x: self.song_ix[x] if x in self.song_ix.keys() else self.song_ix[self.unknown_song_map[x]]).astype(\"category\")\n",
    "        logging.debug(\"preprocess test data in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "    def _fit_model(self, calculate_training_loss, iterations):\n",
    "\n",
    "        start = time.time()\n",
    "        model = AlternatingLeastSquares(factors=self.factors,\n",
    "                                        calculate_training_loss=calculate_training_loss,\n",
    "                                        iterations=iterations)\n",
    "        model.fit(self.targets)\n",
    "\n",
    "        self.item_factors = normalize(model.item_factors)\n",
    "        self.user_factors = normalize(model.user_factors)\n",
    "\n",
    "        pickle.dump(model, open(os.path.join(self.save_dir, 'implicit_model.pkl'), 'wb'))\n",
    "\n",
    "        logging.debug(\"train implicit model in %0.2fs\" % (time.time() - start))\n",
    "\n",
    "    def _get_top(self, model, w, top_n):\n",
    "        result = cosine_similarity(model, model[w].reshape(1, -1)).reshape(1, -1)[0]\n",
    "        return [(i, result[i]) for i in result.argsort()[::-1][:top_n + 1]]\n",
    "\n",
    "    def _get_factors(self, is_train=True):\n",
    "\n",
    "        if is_train:\n",
    "            df = self.train_raw\n",
    "            song_factor = np.array([self.item_factors[i] for i in df.song_ix])\n",
    "            msno_factor = np.array([self.user_factors[i] for i in df.msno_ix])\n",
    "\n",
    "        else:\n",
    "            df = self.test_raw\n",
    "            song_factor = np.array([self.item_factors[i] if i != 'new' else np.full(self.factors, np.nan) for i in df.song_ix])\n",
    "            msno_factor = np.array([self.user_factors[i] if i != 'new' else np.full(self.factors, np.nan) for i in df.msno_ix])\n",
    "\n",
    "        return pd.concat([pd.DataFrame(song_factor), pd.DataFrame(msno_factor)], axis=1, ignore_index=True)\n",
    "\n",
    "    def _get_group(self, is_train=True):\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        if is_train:\n",
    "            df['song_group'] = self.train_raw.song_ix.apply(lambda x: self.item_group[x])\n",
    "            df['msno_group'] = self.train_raw.msno_ix.apply(lambda x: self.user_group[x])\n",
    "        else:\n",
    "            df['song_group'] = self.test_raw.song_ix.apply(lambda x: self.item_group[x] if x != 'new' else np.nan)\n",
    "            df['msno_group'] = self.test_raw.msno_ix.apply(lambda x: self.user_group[x] if x != 'new' else np.nan)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def print_similar_user(self, msno, top_n):\n",
    "\n",
    "        print('-----')\n",
    "        m = self.msno_ix[msno]\n",
    "\n",
    "        for i, v in self._get_top(self.user_factors, m, top_n):\n",
    "            print(\"%0.4f\" % v, self.msno_list[i])\n",
    "\n",
    "        print('-----')\n",
    "\n",
    "    def print_similar_song(self, song_id, top_n):\n",
    "\n",
    "        print('-----')\n",
    "        s = self.song_ix[song_id]\n",
    "\n",
    "        for i, v in self._get_top(self.item_factors, s, top_n):\n",
    "            t = self.song_list[i]\n",
    "            print('%0.4f' % v, self.songs.loc[t]['artist_name'], '-', self.extra.loc[t]['name'])\n",
    "\n",
    "        print('-----')\n",
    "\n",
    "    def get_song_list(self):\n",
    "        return self.song_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cf_lgbm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T10:45:40.108015Z",
     "start_time": "2017-12-07T10:45:40.074476Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
    "params = [{\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.2,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 2 ** 7,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'max_depth': 30,\n",
    "        'metric': 'auc'\n",
    "}, {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'dart',\n",
    "        'learning_rate': 0.2,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 2 ** 7,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'max_depth': 20,\n",
    "        'metric': 'auc'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-07T10:46:42.268Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:46 DEBUG >> Get features ...\n",
      "18:47:04 DEBUG load raw data in 18.27s\n",
      "18:47:32 DEBUG preprocess in 11.00s\n",
      "18:47:37 DEBUG preprocess in 4.71s\n",
      "18:48:22 DEBUG add new features in 30.11s\n",
      "18:48:30 DEBUG add new features in 7.76s\n",
      "18:48:45 DEBUG add comb features in 12.89s\n",
      "18:49:01 DEBUG add comb features in 15.60s\n",
      "18:49:56 DEBUG transform members in 5.15s\n",
      "18:50:28 DEBUG transform songs in 31.45s\n",
      "18:50:29 DEBUG establish known list in 0.71s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno: 2.713816 %\n",
      "msno: 5.455044 %\n",
      "msno: 8.196272 %\n",
      "msno: 10.937500 %\n",
      "msno: 13.678728 %\n",
      "msno: 16.419956 %\n",
      "msno: 19.161184 %\n",
      "msno: 21.902412 %\n",
      "msno: 24.643640 %\n",
      "msno: 27.384868 %\n",
      "msno: 30.126096 %\n",
      "msno: 32.867325 %\n",
      "msno: 35.608553 %\n",
      "msno: 38.349781 %\n",
      "msno: 41.091009 %\n",
      "msno: 43.832237 %\n",
      "msno: 46.573465 %\n",
      "msno: 49.314693 %\n",
      "msno: 52.055921 %\n",
      "msno: 54.797149 %\n",
      "msno: 57.538377 %\n",
      "msno: 60.279605 %\n",
      "msno: 63.020833 %\n",
      "msno: 65.762061 %\n",
      "msno: 68.503289 %\n",
      "msno: 71.244518 %\n",
      "msno: 73.985746 %\n",
      "msno: 76.726974 %\n",
      "msno: 79.468202 %\n",
      "msno: 82.209430 %\n",
      "msno: 84.950658 %\n",
      "msno: 87.691886 %\n",
      "msno: 90.433114 %\n",
      "msno: 93.174342 %\n",
      "msno: 95.915570 %\n",
      "msno: 98.656798 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:51:04 DEBUG transform all unknown data in 35.49s\n",
      "18:51:05 DEBUG preprocess train data in 1.17s\n",
      "18:51:06 DEBUG preprocess test data in 0.52s\n",
      "18:51:11 DEBUG finished iteration 0 in 0.8891937732696533\n",
      "18:51:11 DEBUG loss at iteration 0 is 0.00029049864958589444\n",
      "18:51:12 DEBUG finished iteration 1 in 0.6846601963043213\n",
      "18:51:12 DEBUG loss at iteration 1 is 0.0002634995775192023\n",
      "18:51:13 DEBUG finished iteration 2 in 0.8623757362365723\n",
      "18:51:13 DEBUG loss at iteration 2 is 0.0002583231022678881\n",
      "18:51:14 DEBUG finished iteration 3 in 0.7390546798706055\n",
      "18:51:14 DEBUG loss at iteration 3 is 0.00025680978574791344\n",
      "18:51:14 DEBUG finished iteration 4 in 0.7402968406677246\n",
      "18:51:15 DEBUG loss at iteration 4 is 0.0002563217382206114\n",
      "18:51:15 DEBUG finished iteration 5 in 0.7073028087615967\n",
      "18:51:15 DEBUG loss at iteration 5 is 0.00025609749079101454\n",
      "18:51:16 DEBUG finished iteration 6 in 0.6889891624450684\n",
      "18:51:16 DEBUG loss at iteration 6 is 0.00025596839282133283\n",
      "18:51:17 DEBUG finished iteration 7 in 0.7037944793701172\n",
      "18:51:17 DEBUG loss at iteration 7 is 0.00025588398718471124\n",
      "18:51:18 DEBUG finished iteration 8 in 0.6874375343322754\n",
      "18:51:18 DEBUG loss at iteration 8 is 0.00025582363149022407\n",
      "18:51:19 DEBUG finished iteration 9 in 0.7314362525939941\n",
      "18:51:19 DEBUG loss at iteration 9 is 0.00025577796548648926\n",
      "18:51:20 DEBUG finished iteration 10 in 0.6676754951477051\n",
      "18:51:20 DEBUG loss at iteration 10 is 0.000255742614537915\n",
      "18:51:20 DEBUG finished iteration 11 in 0.7670691013336182\n",
      "18:51:21 DEBUG loss at iteration 11 is 0.0002557153168370194\n",
      "18:51:21 DEBUG finished iteration 12 in 0.8716013431549072\n",
      "18:51:22 DEBUG loss at iteration 12 is 0.00025569453877584663\n",
      "18:51:22 DEBUG finished iteration 13 in 0.6996874809265137\n",
      "18:51:23 DEBUG loss at iteration 13 is 0.0002556789667824169\n",
      "18:51:23 DEBUG finished iteration 14 in 0.6574933528900146\n",
      "18:51:23 DEBUG loss at iteration 14 is 0.00025566741690036856\n",
      "18:51:24 DEBUG finished iteration 15 in 0.7562267780303955\n",
      "18:51:24 DEBUG loss at iteration 15 is 0.0002556588756913092\n",
      "18:51:25 DEBUG finished iteration 16 in 0.8539962768554688\n",
      "18:51:25 DEBUG loss at iteration 16 is 0.0002556525287635886\n",
      "18:51:26 DEBUG finished iteration 17 in 0.7455737590789795\n",
      "18:51:26 DEBUG loss at iteration 17 is 0.0002556477557952178\n",
      "18:51:27 DEBUG finished iteration 18 in 0.7260215282440186\n",
      "18:51:27 DEBUG loss at iteration 18 is 0.0002556440991669589\n",
      "18:51:28 DEBUG finished iteration 19 in 0.7096049785614014\n",
      "18:51:28 DEBUG loss at iteration 19 is 0.000255641230965865\n",
      "18:51:29 DEBUG finished iteration 20 in 0.7048220634460449\n",
      "18:51:29 DEBUG loss at iteration 20 is 0.0002556389170740259\n",
      "18:51:30 DEBUG finished iteration 21 in 0.7908539772033691\n",
      "18:51:30 DEBUG loss at iteration 21 is 0.0002556369931084766\n",
      "18:51:30 DEBUG finished iteration 22 in 0.7266192436218262\n",
      "18:51:31 DEBUG loss at iteration 22 is 0.0002556353406629289\n",
      "18:51:31 DEBUG finished iteration 23 in 0.6730518341064453\n",
      "18:51:31 DEBUG loss at iteration 23 is 0.00025563387574240737\n",
      "18:51:32 DEBUG finished iteration 24 in 0.6564362049102783\n",
      "18:51:32 DEBUG loss at iteration 24 is 0.0002556325365432495\n",
      "18:51:33 DEBUG finished iteration 25 in 0.7500095367431641\n",
      "18:51:33 DEBUG loss at iteration 25 is 0.0002556312783435386\n",
      "18:51:34 DEBUG finished iteration 26 in 0.6708450317382812\n",
      "18:51:34 DEBUG loss at iteration 26 is 0.00025563006685585816\n",
      "18:51:35 DEBUG finished iteration 27 in 0.7365968227386475\n",
      "18:51:35 DEBUG loss at iteration 27 is 0.0002556288770600072\n",
      "18:51:36 DEBUG finished iteration 28 in 0.6980628967285156\n",
      "18:51:36 DEBUG loss at iteration 28 is 0.0002556276894152821\n",
      "18:51:36 DEBUG finished iteration 29 in 0.6797699928283691\n",
      "18:51:37 DEBUG loss at iteration 29 is 0.0002556264890478036\n",
      "18:51:37 DEBUG train implicit model in 31.12s\n",
      "18:55:54 DEBUG add clustering feature in 236.15s\n",
      "/usr/bin/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tvalid_0's auc: 0.740435\n",
      "[10]\tvalid_0's auc: 0.759678\n",
      "[15]\tvalid_0's auc: 0.774074\n",
      "[20]\tvalid_0's auc: 0.781432\n",
      "[25]\tvalid_0's auc: 0.787447\n",
      "[30]\tvalid_0's auc: 0.790796\n",
      "[35]\tvalid_0's auc: 0.793442\n",
      "[40]\tvalid_0's auc: 0.795531\n",
      "[45]\tvalid_0's auc: 0.797847\n",
      "[50]\tvalid_0's auc: 0.800036\n",
      "[55]\tvalid_0's auc: 0.801868\n",
      "[60]\tvalid_0's auc: 0.804234\n",
      "[65]\tvalid_0's auc: 0.805954\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logging.debug('>> Get features ...')\n",
    "    \n",
    "    feature_processor = FeatureProcessor(root='../data')\n",
    "    cf_processor = ImplicitProcessor(feature_size=50,\n",
    "                                     iterations=30,\n",
    "                                     calculate_training_loss=True,\n",
    "                                     save_dir='./model',\n",
    "                                     random_state=50,\n",
    "                                     n_clusters=50,\n",
    "                                     cluster=True)\n",
    "\n",
    "    train, test, unknown_msno_map, unknown_song_map = feature_processor.load()\n",
    "    # train.to_csv('train.csv', index=False)\n",
    "    # test.to_csv('test.csv', index=False)\n",
    "\n",
    "    X_train, y_train, X_test, ids = cf_processor.fit(train_df=train, test_df=test, unknown_msno_map=unknown_msno_map,\n",
    "                                                     unknown_song_map=unknown_song_map)\n",
    "\n",
    "    d_train_final = lgb.Dataset(X_train, y_train)\n",
    "    watchlist_final = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    model_f1 = lgb.train(params[0], train_set=d_train_final,  valid_sets=watchlist_final, num_boost_round=100, verbose_eval=5)\n",
    "    model_f2 = lgb.train(params[1], train_set=d_train_final,  valid_sets=watchlist_final, num_boost_round=100, verbose_eval=5)\n",
    "\n",
    "    print('Making predictions')\n",
    "    p_test_1 = model_f1.predict(X_test)\n",
    "    p_test_2 = model_f2.predict(X_test)\n",
    "    p_test_avg = np.mean([p_test_1, p_test_2], axis=0)\n",
    "\n",
    "    print('Done making predictions')\n",
    "\n",
    "    print('Saving predictions Model model of gbdt')\n",
    "\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = ids\n",
    "    submission['target'] = p_test_avg\n",
    "    # submission['target'] = p_test_1\n",
    "    submission.to_csv('./submit/submission_lgbm_avg.csv.gz', compression='gzip', index=False, float_format='%.5f')\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT, datefmt='%H:%M:%S')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "470px",
    "left": "1389px",
    "right": "20px",
    "top": "114px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
